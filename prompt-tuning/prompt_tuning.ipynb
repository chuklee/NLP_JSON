{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_NAME = \"gpt2\"\n",
    "# MODEL_NAME = \"facebook/opt-350m\"\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10\n",
    "PROMPT_TOKEN = \"[GENERATE]\"\n",
    "MAX_LEN = 1024\n",
    "\n",
    "# Soft Prompt Vocabulary\n",
    "soft_prompt_vocab = [\"[GENERATE]\"]  # Define your custom vocabulary here\n",
    "\n",
    "# Create a word2idx dictionary for the soft prompt vocabulary\n",
    "soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}\n",
    "\n",
    "num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "class GPT2WithSoftPrompt(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_prompts, embedding_size=768):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.soft_prompt = torch.nn.Embedding(num_prompts, embedding_size)\n",
    "\n",
    "    def forward(self, input_ids, prompt_ids):\n",
    "        prompt_embeddings = self.soft_prompt(prompt_ids)\n",
    "        base_embeddings = self.gpt2.transformer.wte(input_ids)\n",
    "        embeddings = torch.cat([prompt_embeddings, base_embeddings.squeeze(0)], dim=0)\n",
    "        outputs = self.gpt2(inputs_embeds=embeddings)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(file_path, num_prompts):\n",
    "    file = open(file_path, \"r\")\n",
    "    \n",
    "    data = json.load(file)\n",
    "    tokenized_inputs = []\n",
    "    tokenized_outputs = []\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    for item in data:\n",
    "        # Adjust the maximum length of articles to avoid exceeding MAX_LEN\n",
    "        max_length_article = MAX_LEN - num_prompts \n",
    "        output_tokens = tokenizer.encode(json.dumps(item[\"output\"]), truncation=True, max_length=max_length_article)\n",
    "        input_tokens = tokenizer.encode(item[\"input\"], truncation=True, max_length=300)\n",
    "\n",
    "        max_length_summary = MAX_LEN\n",
    "        padded_input = input_tokens + [tokenizer.eos_token_id] * (max_length_article - len(input_tokens))\n",
    "        padded_output = output_tokens + [tokenizer.eos_token_id] * (max_length_summary - len(output_tokens))\n",
    "\n",
    "        tokenized_inputs.append(padded_input)\n",
    "        tokenized_outputs.append(padded_output)\n",
    "\n",
    "    file.close()\n",
    "    \n",
    "    train_limit = int(len(tokenized_inputs) * 0.7)\n",
    "    val_limit = int(len(tokenized_inputs) * 0.9)\n",
    "\n",
    "    return tokenized_inputs[:train_limit], tokenized_outputs[:train_limit], tokenized_inputs[train_limit:val_limit], tokenized_outputs[train_limit:val_limit], tokenized_inputs[val_limit:], tokenized_outputs[val_limit:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenized_inputs_train, tokenized_outputs_train, tokenized_inputs_validation, tokenized_outputs_validation, tokenized_inputs_test, tokenized_outputs_test = load_and_preprocess_data(\"dataset.json\", num_prompts)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model Initialization\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_inputs_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_outputs_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "GRADIENT_CLIP_NORM = 1.0\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "prompt_id = prompt_id.to(device)\n",
    "\n",
    "\n",
    "def fine_tune_on_summarization(model, train_inputs, train_outputs, val_inputs, val_outputs, test_inputs, test_outputs):\n",
    "    optimizer = torch.optim.Adam(model.soft_prompt.parameters())\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        # Gradient accumulation initialization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        accumulated_loss = 0\n",
    "        loss = 0\n",
    "\n",
    "        # Use tqdm for progress bar\n",
    "        with tqdm(enumerate(zip(train_inputs, train_outputs)), total=len(train_inputs), desc=f\"Epoch {epoch + 1}/{EPOCHS}\", unit=\"batch\") as progress:\n",
    "            train_percentage_matched = 0\n",
    "            train_percentage_matched_ct = 0\n",
    "\n",
    "            for idx, (input, output) in progress:\n",
    "                input_ids = torch.tensor(input).to(device)\n",
    "                labels = torch.tensor(output).to(device)\n",
    "                outputs = model(input_ids, prompt_id)\n",
    "\n",
    "                ignore_index = tokenizer.eos_token_id\n",
    "                loss += CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "\n",
    "                # Metrics\n",
    "                set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                set2 = set(labels.cpu().numpy())\n",
    "\n",
    "                # Calculate the intersection of sets\n",
    "                intersection = set1.intersection(set2)\n",
    "\n",
    "                # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "                percentage = (len(intersection) / len(set1)) * 100\n",
    "                train_percentage_matched += percentage\n",
    "                train_percentage_matched_ct += 1\n",
    "\n",
    "                # Backpropagate losses every GRADIENT_ACCUMULATION_STEPS or at the end of the dataset\n",
    "                if (idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or idx == len(train_inputs) - 1:\n",
    "                    (loss / GRADIENT_ACCUMULATION_STEPS).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_NORM)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = 0\n",
    "            \n",
    "            print(\"Train : % Exact Match: \",train_percentage_matched/train_percentage_matched_ct)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_percentage_matched = 0\n",
    "            val_percentage_matched_ct = 0\n",
    "\n",
    "            for input, output in tqdm(zip(val_inputs, val_outputs), total=len(val_inputs), desc=\"Validation\", unit=\"batch\"):\n",
    "                input_ids = torch.tensor(input).to(device)\n",
    "                labels = torch.tensor(output).to(device)\n",
    "                outputs = model(input_ids, prompt_id)\n",
    "\n",
    "                ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n",
    "                val_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                # Metrics\n",
    "                set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                set2 = set(labels.cpu().numpy())\n",
    "\n",
    "                # Calculate the intersection of sets\n",
    "                intersection = set1.intersection(set2)\n",
    "\n",
    "                # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "                percentage = (len(intersection) / len(set1)) * 100\n",
    "                val_percentage_matched += percentage\n",
    "                val_percentage_matched_ct += 1\n",
    "\n",
    "        print(\"Val : % Exact Match: \",val_percentage_matched/val_percentage_matched_ct)\n",
    "        avg_val_loss = total_val_loss / len(val_inputs)\n",
    "        print(\"Val Loss : \",avg_val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "            if no_improvement_epochs >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_percentage_matched = 0\n",
    "        test_percentage_matched_ct = 0\n",
    "\n",
    "        for input, output in tqdm(zip(test_inputs, test_outputs), total=len(test_inputs), desc=\"Test\", unit=\"batch\"):\n",
    "            input_ids = torch.tensor(input).to(device)\n",
    "            labels = torch.tensor(output).to(device)\n",
    "            outputs = model(input_ids, prompt_id)\n",
    "\n",
    "            ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n",
    "            test_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "            # Metrics\n",
    "            set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            set2 = set(labels.cpu().numpy())\n",
    "\n",
    "            # Calculate the intersection of sets\n",
    "            intersection = set1.intersection(set2)\n",
    "\n",
    "            # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "            percentage = (len(intersection) / len(set1)) * 100\n",
    "            test_percentage_matched += percentage\n",
    "            test_percentage_matched_ct += 1\n",
    "        \n",
    "        \n",
    "        print(\"Test : % Exact Match: \",test_percentage_matched/test_percentage_matched_ct)\n",
    "        avg_test_loss = total_test_loss / len(test_inputs)\n",
    "        print(\"Test Loss : \",avg_test_loss)\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|▏         | 3/219 [00:15<18:58,  5.27s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fine_tuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune_on_summarization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_inputs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_outputs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_inputs_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_outputs_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_inputs_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_outputs_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 39\u001b[0m, in \u001b[0;36mfine_tune_on_summarization\u001b[1;34m(model, train_inputs, train_outputs, val_inputs, val_outputs, test_inputs, test_outputs)\u001b[0m\n\u001b[0;32m     36\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m CrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mignore_index)(outputs\u001b[38;5;241m.\u001b[39mlogits, labels)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m set1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     40\u001b[0m set2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Calculate the intersection of sets\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fine_tuned_model = fine_tune_on_summarization(model, tokenized_inputs_train, tokenized_outputs_train, tokenized_inputs_validation, tokenized_outputs_validation, tokenized_inputs_test, tokenized_outputs_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(fine_tuned_model.state_dict(), 'fine_tuned_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samya\\AppData\\Local\\Temp\\ipykernel_14628\\1971522244.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('fine_tuned_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2WithSoftPrompt(\n",
       "  (gpt2): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2SdpaAttention(\n",
       "            (c_attn): Conv1D(nf=2304, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D(nf=3072, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=3072)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       "  (soft_prompt): Embedding(1, 768)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a new instance of the model\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n",
    "\n",
    "# Load the saved model state_dict\n",
    "model.load_state_dict(torch.load('fine_tuned_model.pth'))\n",
    "\n",
    "# Make sure the model is in evaluation mode after loading\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Input text for summarization\n",
    "input_text = \"Transform into JSON including 'multi_cloud_controller', 'orchestration_policies', 'migration_strategies', and 'cost_optimizations': 'CloudHarmonizer managed 2 orchestration policies, used live migration and backup-restore strategies, optimizing costs by 15%.'\"\n",
    "\n",
    "# Tokenize and encode the input text\n",
    "input_ids = tokenizer.encode(input_text, truncation=True, max_length=1024)\n",
    "\n",
    "# Convert the input_ids to a PyTorch tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "# Generate a summary\n",
    "with torch.no_grad():\n",
    "    # Assuming single prompt\n",
    "    outputs = model(input_ids.to(device), prompt_ids=prompt_id.to(device))\n",
    "    pred_logits = outputs.logits\n",
    "    print(pred_logits.shape)\n",
    "\n",
    "\n",
    "# Get the token IDs with the highest probability for each position\n",
    "predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "\n",
    "# Convert token IDs into words using the tokenizer\n",
    "predicted_tokens = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n. the. the ,-tscloud_ 'multi 'stra_controllerredicties_ 'multi__mategyies_ ' 'm_strized', 'msugeon___.stration_ ' ' to_, ',optimore,, used the, '% to\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Input text for summarization\n",
    "input_text = \"Convert the following sentence into a JSON object with clear key-value pairs: 'I bought 2 flowers and a flower pot.'\"\n",
    "\n",
    "# Tokenize and encode the input text\n",
    "input_ids = tokenizer.encode(input_text, truncation=True, max_length=1024)\n",
    "\n",
    "# Convert the input_ids to a PyTorch tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "# Generate a summary\n",
    "with torch.no_grad():\n",
    "    # Assuming single prompt\n",
    "    outputs = model(input_ids.to(device), prompt_ids=prompt_id.to(device))\n",
    "    pred_logits = outputs.logits\n",
    "    print(pred_logits.shape)\n",
    "\n",
    "\n",
    "# Get the token IDs with the highest probability for each position\n",
    "predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "\n",
    "# Convert token IDs into words using the tokenizer\n",
    "predicted_tokens = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n.. same day: the list string: the and:to::\\n{'m a.: then new::\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
