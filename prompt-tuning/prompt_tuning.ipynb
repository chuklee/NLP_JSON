{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:08:54.911849Z",
     "iopub.status.busy": "2024-12-10T18:08:54.911509Z",
     "iopub.status.idle": "2024-12-10T18:09:03.202856Z",
     "shell.execute_reply": "2024-12-10T18:09:03.201989Z",
     "shell.execute_reply.started": "2024-12-10T18:08:54.911819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:09:03.205876Z",
     "iopub.status.busy": "2024-12-10T18:09:03.205476Z",
     "iopub.status.idle": "2024-12-10T18:09:03.210922Z",
     "shell.execute_reply": "2024-12-10T18:09:03.210049Z",
     "shell.execute_reply.started": "2024-12-10T18:09:03.205833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:09:03.212304Z",
     "iopub.status.busy": "2024-12-10T18:09:03.211976Z",
     "iopub.status.idle": "2024-12-10T18:09:03.496880Z",
     "shell.execute_reply": "2024-12-10T18:09:03.495961Z",
     "shell.execute_reply.started": "2024-12-10T18:09:03.212272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_NAME = \"gpt2\"\n",
    "# MODEL_NAME = \"facebook/opt-350m\"\n",
    "PROMPT_TOKEN = \"[GENERATE] [JSON] [OBJECT] [MODEL] [FORMAT] [KEY] [VALUE] [FIELD]\"\n",
    "MAX_LEN = 1024\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 100\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "GRADIENT_CLIP_NORM = 1.0\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Soft Prompt Vocabulary\n",
    "soft_prompt_vocab = [\"[GENERATE]\", \"[JSON]\", \"[OBJECT]\", \"[MODEL]\", \"[FORMAT]\", \"[KEY]\", \"[VALUE]\", \"[FIELD]\"]  # Define your custom vocabulary here\n",
    "\n",
    "# Create a word2idx dictionary for the soft prompt vocabulary\n",
    "soft_prompt_word2idx = {word: idx for idx, word in enumerate(soft_prompt_vocab)}\n",
    "\n",
    "num_prompts = len([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "prompt_id = torch.tensor([soft_prompt_word2idx[word] for word in PROMPT_TOKEN.split()])\n",
    "prompt_id = prompt_id.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:09:03.498597Z",
     "iopub.status.busy": "2024-12-10T18:09:03.498211Z",
     "iopub.status.idle": "2024-12-10T18:09:03.504979Z",
     "shell.execute_reply": "2024-12-10T18:09:03.503987Z",
     "shell.execute_reply.started": "2024-12-10T18:09:03.498554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "class GPT2WithSoftPrompt(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_prompts, embedding_size=768):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.soft_prompt = torch.nn.Embedding(num_prompts, embedding_size)\n",
    "\n",
    "    def forward(self, input_ids, prompt_ids):\n",
    "        prompt_embeddings = self.soft_prompt(prompt_ids)\n",
    "        base_embeddings = self.gpt2.transformer.wte(input_ids)\n",
    "        embeddings = torch.cat([prompt_embeddings, base_embeddings.squeeze(0)], dim=0)\n",
    "        outputs = self.gpt2(inputs_embeds=embeddings)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:09:03.507578Z",
     "iopub.status.busy": "2024-12-10T18:09:03.507309Z",
     "iopub.status.idle": "2024-12-10T18:09:03.515616Z",
     "shell.execute_reply": "2024-12-10T18:09:03.514979Z",
     "shell.execute_reply.started": "2024-12-10T18:09:03.507551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(file_path, num_prompts):\n",
    "    file = open(file_path, \"r\")\n",
    "    \n",
    "    data = json.load(file)\n",
    "    tokenized_inputs = []\n",
    "    tokenized_outputs = []\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    for item in data:\n",
    "        # Adjust the maximum length of articles to avoid exceeding MAX_LEN\n",
    "        max_length_article = MAX_LEN - num_prompts \n",
    "        output_tokens = tokenizer.encode(json.dumps(item[\"output\"]), truncation=True, max_length=max_length_article)\n",
    "        input_tokens = tokenizer.encode(item[\"input\"], truncation=True, max_length=300)\n",
    "\n",
    "        max_length_summary = MAX_LEN\n",
    "        padded_input = input_tokens + [tokenizer.eos_token_id] * (max_length_article - len(input_tokens))\n",
    "        padded_output = output_tokens + [tokenizer.eos_token_id] * (max_length_summary - len(output_tokens))\n",
    "\n",
    "        tokenized_inputs.append(padded_input)\n",
    "        tokenized_outputs.append(padded_output)\n",
    "\n",
    "    file.close()\n",
    "    \n",
    "    train_limit = int(len(tokenized_inputs) * 0.7)\n",
    "    val_limit = int(len(tokenized_inputs) * 0.9)\n",
    "\n",
    "    return tokenized_inputs[:train_limit], tokenized_outputs[:train_limit], tokenized_inputs[train_limit:val_limit], tokenized_outputs[train_limit:val_limit], tokenized_inputs[val_limit:], tokenized_outputs[val_limit:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:09:03.516782Z",
     "iopub.status.busy": "2024-12-10T18:09:03.516549Z",
     "iopub.status.idle": "2024-12-10T18:09:10.101082Z",
     "shell.execute_reply": "2024-12-10T18:09:10.100375Z",
     "shell.execute_reply.started": "2024-12-10T18:09:03.516757Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75bafe2c74f430284f208aee8cee849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27355a79676340a59bf81abae9e2251f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28b3594ee294db5b66a37bfed52659a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810c06bf7fa148e8a1f55f75a868ae01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e2c8a872b648b8824ae320daef1b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617e775f67ac4cd19e5a5c96a3080b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e898f54e77148669d7bfe0653e60be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenized_inputs_train, tokenized_outputs_train, tokenized_inputs_validation, tokenized_outputs_validation, tokenized_inputs_test, tokenized_outputs_test = load_and_preprocess_data(\"/kaggle/input/json-dataset/dataset.json\", num_prompts)\n",
    "\n",
    "# Model Initialization\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:09:10.102295Z",
     "iopub.status.busy": "2024-12-10T18:09:10.102060Z",
     "iopub.status.idle": "2024-12-10T18:09:10.107847Z",
     "shell.execute_reply": "2024-12-10T18:09:10.107004Z",
     "shell.execute_reply.started": "2024-12-10T18:09:10.102271Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1016"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_inputs_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:09:10.109279Z",
     "iopub.status.busy": "2024-12-10T18:09:10.108991Z",
     "iopub.status.idle": "2024-12-10T18:09:10.117829Z",
     "shell.execute_reply": "2024-12-10T18:09:10.117139Z",
     "shell.execute_reply.started": "2024-12-10T18:09:10.109254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_outputs_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:09:10.119320Z",
     "iopub.status.busy": "2024-12-10T18:09:10.119014Z",
     "iopub.status.idle": "2024-12-10T18:09:10.140049Z",
     "shell.execute_reply": "2024-12-10T18:09:10.139068Z",
     "shell.execute_reply.started": "2024-12-10T18:09:10.119295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fine_tune_on_summarization(model, train_inputs, train_outputs, val_inputs, val_outputs, test_inputs, test_outputs):\n",
    "    optimizer = torch.optim.Adam(model.soft_prompt.parameters())\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        # Gradient accumulation initialization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        accumulated_loss = 0\n",
    "        loss = 0\n",
    "\n",
    "        # Use tqdm for progress bar\n",
    "        with tqdm(enumerate(zip(train_inputs, train_outputs)), total=len(train_inputs), desc=f\"Epoch {epoch + 1}/{EPOCHS}\", unit=\"batch\") as progress:\n",
    "            train_percentage_matched = 0\n",
    "            train_percentage_matched_ct = 0\n",
    "\n",
    "            for idx, (input, output) in progress:\n",
    "                input_ids = torch.tensor(input).to(device)\n",
    "                labels = torch.tensor(output).to(device)\n",
    "                outputs = model(input_ids, prompt_id)\n",
    "\n",
    "                ignore_index = tokenizer.eos_token_id\n",
    "                loss += CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "\n",
    "                # Metrics\n",
    "                set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                set2 = set(labels.cpu().numpy())\n",
    "\n",
    "                # Calculate the intersection of sets\n",
    "                intersection = set1.intersection(set2)\n",
    "\n",
    "                # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "                percentage = (len(intersection) / len(set1)) * 100\n",
    "                train_percentage_matched += percentage\n",
    "                train_percentage_matched_ct += 1\n",
    "\n",
    "                # Backpropagate losses every GRADIENT_ACCUMULATION_STEPS or at the end of the dataset\n",
    "                if (idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or idx == len(train_inputs) - 1:\n",
    "                    (loss / GRADIENT_ACCUMULATION_STEPS).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_NORM)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = 0\n",
    "            \n",
    "            print(\"Train : % Exact Match: \",train_percentage_matched/train_percentage_matched_ct)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_percentage_matched = 0\n",
    "            val_percentage_matched_ct = 0\n",
    "\n",
    "            for input, output in tqdm(zip(val_inputs, val_outputs), total=len(val_inputs), desc=\"Validation\", unit=\"batch\"):\n",
    "                input_ids = torch.tensor(input).to(device)\n",
    "                labels = torch.tensor(output).to(device)\n",
    "                outputs = model(input_ids, prompt_id)\n",
    "\n",
    "                ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n",
    "                val_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                # Metrics\n",
    "                set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                set2 = set(labels.cpu().numpy())\n",
    "\n",
    "                # Calculate the intersection of sets\n",
    "                intersection = set1.intersection(set2)\n",
    "\n",
    "                # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "                percentage = (len(intersection) / len(set1)) * 100\n",
    "                val_percentage_matched += percentage\n",
    "                val_percentage_matched_ct += 1\n",
    "\n",
    "        print(\"Val : % Exact Match: \",val_percentage_matched/val_percentage_matched_ct)\n",
    "        avg_val_loss = total_val_loss / len(val_inputs)\n",
    "        print(\"Val Loss : \",avg_val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "            if no_improvement_epochs >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping after {EARLY_STOPPING_PATIENCE} epochs without improvement.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_percentage_matched = 0\n",
    "        test_percentage_matched_ct = 0\n",
    "\n",
    "        for input, output in tqdm(zip(test_inputs, test_outputs), total=len(test_inputs), desc=\"Test\", unit=\"batch\"):\n",
    "            input_ids = torch.tensor(input).to(device)\n",
    "            labels = torch.tensor(output).to(device)\n",
    "            outputs = model(input_ids, prompt_id)\n",
    "\n",
    "            ignore_index = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else -100\n",
    "            test_loss = CrossEntropyLoss(ignore_index=ignore_index)(outputs.logits, labels)\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "            # Metrics\n",
    "            set1 = set(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            set2 = set(labels.cpu().numpy())\n",
    "\n",
    "            # Calculate the intersection of sets\n",
    "            intersection = set1.intersection(set2)\n",
    "\n",
    "            # Calculate the percentage of indices in the first tensor that are also in the second tensor\n",
    "            percentage = (len(intersection) / len(set1)) * 100\n",
    "            test_percentage_matched += percentage\n",
    "            test_percentage_matched_ct += 1\n",
    "        \n",
    "        \n",
    "        print(\"Test : % Exact Match: \",test_percentage_matched/test_percentage_matched_ct)\n",
    "        avg_test_loss = total_test_loss / len(test_inputs)\n",
    "        print(\"Test Loss : \",avg_test_loss)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:09:10.141291Z",
     "iopub.status.busy": "2024-12-10T18:09:10.141018Z",
     "iopub.status.idle": "2024-12-10T18:45:28.202985Z",
     "shell.execute_reply": "2024-12-10T18:45:28.201988Z",
     "shell.execute_reply.started": "2024-12-10T18:09:10.141250Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 219/219 [01:00<00:00,  3.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  11.7906166180054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:05<00:00, 10.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  18.33029597537911\n",
      "Val Loss :  10.20312052388345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 219/219 [01:04<00:00,  3.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  12.238617174067748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  17.25844274293789\n",
      "Val Loss :  9.510851798519012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 219/219 [01:06<00:00,  3.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  12.47101623034656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00, 10.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  16.583403155983795\n",
      "Val Loss :  9.065150753144295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  12.21869909420271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00, 10.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  14.64771787352432\n",
      "Val Loss :  8.903754495805309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 219/219 [01:06<00:00,  3.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  13.938403251987372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00, 10.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  14.318781576846092\n",
      "Val Loss :  8.707564446233935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  18.790590471694422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  25.36279133053326\n",
      "Val Loss :  8.507593539453321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 219/219 [01:06<00:00,  3.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  21.4263839596763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00, 10.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  25.492764930620712\n",
      "Val Loss :  8.288107718190838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 219/219 [01:06<00:00,  3.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  23.933052196761416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00, 10.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  27.372295386626753\n",
      "Val Loss :  8.020360592872866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 219/219 [01:06<00:00,  3.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  27.270510163453746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  33.47479300746305\n",
      "Val Loss :  7.799670411694434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  30.434922564794444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  33.14604526298074\n",
      "Val Loss :  7.433152298773488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  33.64147570522943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  39.65439130761711\n",
      "Val Loss :  7.209127272329023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  35.34206874133288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  40.90342005664586\n",
      "Val Loss :  7.031517736373409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  35.88338730318878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  43.76472094214028\n",
      "Val Loss :  6.848230392702164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 219/219 [01:06<00:00,  3.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  37.73788940489176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  30.574264445232178\n",
      "Val Loss :  6.646610060045796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  38.04727020384557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  28.879722428109517\n",
      "Val Loss :  6.536799384701636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  38.90227813506999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  29.43666548505257\n",
      "Val Loss :  6.462057744303057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 219/219 [01:06<00:00,  3.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  38.86832818652441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  26.838630613535745\n",
      "Val Loss :  6.394574619108631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  41.345839714992316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  25.13507326007324\n",
      "Val Loss :  6.359844053945234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 219/219 [01:06<00:00,  3.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  41.54452543559373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  38.71314288178616\n",
      "Val Loss :  6.327132878764983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 219/219 [01:06<00:00,  3.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  43.06228028168101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  38.6166253101737\n",
      "Val Loss :  6.300835094144268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  42.72557237557872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  37.798062152900876\n",
      "Val Loss :  6.269700127263223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 219/219 [01:06<00:00,  3.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  42.83895863511198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  36.849521446295654\n",
      "Val Loss :  6.237122235759612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  42.98330573559592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  37.78469514356612\n",
      "Val Loss :  6.210632908728815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  41.85096150854874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  39.048652959943325\n",
      "Val Loss :  6.195588173404817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 219/219 [01:06<00:00,  3.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  42.921311360759596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  38.426331732783346\n",
      "Val Loss :  6.204828654566119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  42.509137650830596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  38.8481545336384\n",
      "Val Loss :  6.170154448478453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 219/219 [01:06<00:00,  3.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  42.453235085344126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  39.51298701298702\n",
      "Val Loss :  6.159498145503383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 219/219 [01:06<00:00,  3.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  42.9109562815933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  37.726390465954054\n",
      "Val Loss :  6.141156435012817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|██████████| 219/219 [01:06<00:00,  3.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  42.81106757208373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  36.682409936679406\n",
      "Val Loss :  6.1450788513306644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|██████████| 219/219 [01:06<00:00,  3.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : % Exact Match:  41.65031549918521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 62/62 [00:06<00:00,  9.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : % Exact Match:  36.45815087821731\n",
      "Val Loss :  6.142245631064138\n",
      "Early stopping after 2 epochs without improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 32/32 [00:03<00:00, 10.25batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test : % Exact Match:  34.22540771116138\n",
      "Test Loss :  7.36285375058651\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = fine_tune_on_summarization(model, tokenized_inputs_train, tokenized_outputs_train, tokenized_inputs_validation, tokenized_outputs_validation, tokenized_inputs_test, tokenized_outputs_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:45:28.204717Z",
     "iopub.status.busy": "2024-12-10T18:45:28.204412Z",
     "iopub.status.idle": "2024-12-10T18:45:28.933613Z",
     "shell.execute_reply": "2024-12-10T18:45:28.932649Z",
     "shell.execute_reply.started": "2024-12-10T18:45:28.204687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(fine_tuned_model.state_dict(), 'fine_tuned_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:45:28.935055Z",
     "iopub.status.busy": "2024-12-10T18:45:28.934757Z",
     "iopub.status.idle": "2024-12-10T18:45:30.042726Z",
     "shell.execute_reply": "2024-12-10T18:45:30.041901Z",
     "shell.execute_reply.started": "2024-12-10T18:45:28.935027Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1971522244.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('fine_tuned_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2WithSoftPrompt(\n",
       "  (gpt2): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2SdpaAttention(\n",
       "            (c_attn): Conv1D(nf=2304, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D(nf=3072, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=3072)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       "  (soft_prompt): Embedding(8, 768)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a new instance of the model\n",
    "model = GPT2WithSoftPrompt(MODEL_NAME, num_prompts).to(device)\n",
    "\n",
    "# Load the saved model state_dict\n",
    "model.load_state_dict(torch.load('fine_tuned_model.pth'))\n",
    "\n",
    "# Make sure the model is in evaluation mode after loading\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:45:30.043890Z",
     "iopub.status.busy": "2024-12-10T18:45:30.043655Z",
     "iopub.status.idle": "2024-12-10T18:45:30.075991Z",
     "shell.execute_reply": "2024-12-10T18:45:30.075138Z",
     "shell.execute_reply.started": "2024-12-10T18:45:30.043867Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Input text for summarization\n",
    "input_text = \"Transform into JSON including 'multi_cloud_controller', 'orchestration_policies', 'migration_strategies', and 'cost_optimizations': 'CloudHarmonizer managed 2 orchestration policies, used live migration and backup-restore strategies, optimizing costs by 15%.'\"\n",
    "\n",
    "# Tokenize and encode the input text\n",
    "input_ids = tokenizer.encode(input_text, truncation=True, max_length=1024)\n",
    "\n",
    "# Convert the input_ids to a PyTorch tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "# Generate a summary\n",
    "with torch.no_grad():\n",
    "    # Assuming single prompt\n",
    "    outputs = model(input_ids.to(device), prompt_ids=prompt_id.to(device))\n",
    "    pred_logits = outputs.logits\n",
    "    print(pred_logits.shape)\n",
    "\n",
    "\n",
    "# Get the token IDs with the highest probability for each position\n",
    "predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "\n",
    "# Convert token IDs into words using the tokenizer\n",
    "predicted_tokens = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:45:30.079935Z",
     "iopub.status.busy": "2024-12-10T18:45:30.079654Z",
     "iopub.status.idle": "2024-12-10T18:45:30.085334Z",
     "shell.execute_reply": "2024-12-10T18:45:30.084421Z",
     "shell.execute_reply.started": "2024-12-10T18:45:30.079909Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"mer\": \"\":\",\",\":\":\":\": \"\":\":\":\":\":\": \"\":\":\":\":\":ity\": \" \"\":\":\":\":\":\": \"\": \"\":\":\":\": \" \"\":\":\":\":\":\": \"\": \"\": \"\": \"\": \"\": \"\": \"\": \"\":\": \" \" \" \"'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:45:30.086725Z",
     "iopub.status.busy": "2024-12-10T18:45:30.086441Z",
     "iopub.status.idle": "2024-12-10T18:45:30.105535Z",
     "shell.execute_reply": "2024-12-10T18:45:30.104768Z",
     "shell.execute_reply.started": "2024-12-10T18:45:30.086698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Input text for summarization\n",
    "input_text = \"Convert the following sentence into a JSON object with clear key-value pairs: 'I bought 2 flowers and a flower pot.'\"\n",
    "\n",
    "# Tokenize and encode the input text\n",
    "input_ids = tokenizer.encode(input_text, truncation=True, max_length=1024)\n",
    "\n",
    "# Convert the input_ids to a PyTorch tensor\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "# Generate a summary\n",
    "with torch.no_grad():\n",
    "    # Assuming single prompt\n",
    "    outputs = model(input_ids.to(device), prompt_ids=prompt_id.to(device))\n",
    "    pred_logits = outputs.logits\n",
    "    print(pred_logits.shape)\n",
    "\n",
    "\n",
    "# Get the token IDs with the highest probability for each position\n",
    "predicted_token_ids = torch.argmax(pred_logits, dim=-1)\n",
    "\n",
    "# Convert token IDs into words using the tokenizer\n",
    "predicted_tokens = tokenizer.decode(predicted_token_ids.squeeze(0), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:45:30.106772Z",
     "iopub.status.busy": "2024-12-10T18:45:30.106537Z",
     "iopub.status.idle": "2024-12-10T18:45:30.111753Z",
     "shell.execute_reply": "2024-12-10T18:45:30.110840Z",
     "shell.execute_reply.started": "2024-12-10T18:45:30.106749Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"mer\": \"\":\",\",\":\":\":\":\":\":\":\":\":\":\":\":\":\":\": \" \"\":\": \"\":\":\":\": \"\":'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tokens"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6249653,
     "sourceId": 10127265,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
